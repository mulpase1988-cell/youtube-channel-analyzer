# ========================================
# ê¸€ë¡œë²Œ í•«ë°ì´í„° ìˆ˜ì§‘ê¸° - Turso DB ë²„ì „
# ========================================

import requests
import re
import time
import gspread
import os
import json
import tempfile
from google.oauth2.service_account import Credentials
from datetime import datetime

# ========================================
# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œ
# ========================================
SERVICE_ACCOUNT_JSON = os.environ.get('GOOGLE_SERVICE_ACCOUNT')
TURSO_URL = os.environ.get('TURSO_URL')
TURSO_TOKEN = os.environ.get('TURSO_TOKEN')

if not SERVICE_ACCOUNT_JSON or not TURSO_URL or not TURSO_TOKEN:
    raise Exception("âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: GOOGLE_SERVICE_ACCOUNT, TURSO_URL, TURSO_TOKEN")

# JSONì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:
    f.write(SERVICE_ACCOUNT_JSON)
    SERVICE_ACCOUNT_FILE = f.name

SHEET_NAME = os.environ.get('SHEET_NAME', 'ìœ íŠœë¸Œë³´ë¬¼ì°½ê³ _í…ŒìŠ¤íŠ¸')

# ========================================
# Turso ì¿¼ë¦¬ í•¨ìˆ˜
# ========================================
def execute_turso_query(sql, args=None):
    """Turso DBì— ì¿¼ë¦¬ ì‹¤í–‰"""
    headers = {
        'Authorization': f'Bearer {TURSO_TOKEN}',
        'Content-Type': 'application/json'
    }
    
    turso_api_url = TURSO_URL.replace('libsql://', 'https://') + '/v2/pipeline'
    
    payload = {
        'requests': [
            {
                'type': 'execute',
                'stmt': {
                    'sql': sql,
                    'args': args if args else []
                }
            }
        ]
    }
    
    response = requests.post(turso_api_url, json=payload, headers=headers)
    
    if response.status_code != 200:
        raise Exception(f"Turso ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {response.text}")
    
    return response.json()

def insert_to_turso(data_rows):
    """Turso DBì— ë°ì´í„° ì‚½ì…"""
    if not data_rows:
        return 0
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    execute_turso_query("DELETE FROM global_hot_data")
    
    # ìƒˆ ë°ì´í„° ì‚½ì…
    inserted = 0
    for row in data_rows:
        try:
            sql = """
            INSERT INTO global_hot_data 
            (collect_datetime, country, category, detail_type, ranking, thumbnail, 
             video_title, view_count, channel_name, handle, subscriber_count, tags, 
             video_link, channel_id, thumbnail_url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            # â† ì—¬ê¸°ì„œë¶€í„° ì¶”ê°€
            args = [
                row[0],                          # collect_datetime
                row[1],                          # country
                row[2],                          # category
                row[3],                          # detail_type
                int(row[4]) if row[4] else 0,   # ranking (ì •ìˆ˜)
                row[5],                          # thumbnail
                row[6],                          # video_title
                int(row[7]) if row[7] else 0,   # view_count (ì •ìˆ˜)
                row[8],                          # channel_name
                row[9],                          # handle
                int(row[10]) if row[10] else 0, # subscriber_count (ì •ìˆ˜)
                row[11],                         # tags
                row[12],                         # video_link
                row[13],                         # channel_id
                row[14]                          # thumbnail_url
            ]
            # â† ì—¬ê¸°ê¹Œì§€ ì¶”ê°€
            
            execute_turso_query(sql, args)  # â† rowë¥¼ argsë¡œ ë³€ê²½
            inserted += 1
        except Exception as e:
            print(f"âš ï¸  í–‰ ì‚½ì… ì‹¤íŒ¨: {str(e)}")
    
    return inserted


# ========================================
# í—¬í¼ í•¨ìˆ˜
# ========================================
def parse_duration(duration):
    """YouTube durationì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
    hours = re.search(r'(\d+)H', duration)
    minutes = re.search(r'(\d+)M', duration)
    seconds = re.search(r'(\d+)S', duration)
    return (int(hours.group(1)) * 3600 if hours else 0) + \
           (int(minutes.group(1)) * 60 if minutes else 0) + \
           (int(seconds.group(1)) if seconds else 0)

# ========================================
# ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ========================================
def run_final_collector():
    """ê¸€ë¡œë²Œ í•«ë°ì´í„° ìˆ˜ì§‘ ë° Turso DBì— ì €ì¥"""
    print("=" * 60)
    print("ğŸ”¥ ê¸€ë¡œë²Œ í•«ë°ì´í„° ìˆ˜ì§‘ê¸° v2.0 (Turso DB)")
    print("=" * 60)
    print(f"ğŸš€ ìˆ˜ì§‘ ì‹œì‘ (ë¯¸ë“œí¼ 2ë¶„ ë¯¸ë§Œ / íƒœê·¸ ê³µë°± ì²˜ë¦¬)\n")
    
    try:
        # Google Sheets ì—°ê²°
        print("ğŸ“Š Google Sheets ì—°ê²° ì¤‘...")
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]
        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)
        client = gspread.authorize(creds)
        spreadsheet = client.open(SHEET_NAME)
        print(f"âœ… '{SHEET_NAME}' ì—°ê²° ì™„ë£Œ\n")
        
        # API í‚¤ ë¡œë“œ
        print("ğŸ”‘ API í‚¤ ë¡œë“œ ì¤‘...")
        api_sheet = spreadsheet.worksheet('API_í‚¤_ê´€ë¦¬')
        api_data = api_sheet.get_all_values()[3:]
        active_keys = [row[2].strip() for row in api_data if len(row) > 2 and row[2].strip().startswith('AIza')]
        print(f"âœ… í™œì„± API í‚¤ {len(active_keys)}ê°œ ë¡œë“œ\n")

        if not active_keys:
            print("âŒ í™œì„±í™”ëœ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # êµ­ê°€ ì„¤ì • ë¡œë“œ
        print("ğŸŒ êµ­ê°€ ì„¤ì • ë¡œë“œ ì¤‘...")
        countries = [
            d for d in spreadsheet.worksheet('ì„¤ì •_êµ­ê°€').get_all_records() 
            if str(d.get('ìˆ˜ì§‘ì—¬ë¶€')).upper() == 'Y'
        ]
        print(f"âœ… ìˆ˜ì§‘ ëŒ€ìƒ êµ­ê°€: {len(countries)}ê°œ")
        for c in countries:
            print(f"   - {c['êµ­ê°€ëª…']} ({c['êµ­ê°€ì½”ë“œ']})")
        
        # ì¹´í…Œê³ ë¦¬ ì„¤ì • ë¡œë“œ
        print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„¤ì • ë¡œë“œ ì¤‘...")
        categories = [
            d for d in spreadsheet.worksheet('ì„¤ì •_ì¹´í…Œê³ ë¦¬').get_all_records() 
            if str(d.get('ìˆ˜ì§‘ì—¬ë¶€')).upper() == 'Y'
        ]
        print(f"âœ… ìˆ˜ì§‘ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: {len(categories)}ê°œ")
        for cat in categories:
            print(f"   - {cat['ì¹´í…Œê³ ë¦¬ëª…']} (ID: {cat['ì¹´í…Œê³ ë¦¬ID']})")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š ì´ {len(countries)} Ã— {len(categories)} = {len(countries) * len(categories)}ê°œ ì¡°í•© ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60 + "\n")
        
        all_results = []
        key_idx = 0
        success_count = 0
        fail_count = 0

        for country_idx, country in enumerate(countries, 1):
            for cat_idx, cat in enumerate(categories, 1):
                current_key = active_keys[key_idx % len(active_keys)]
                combo_num = (country_idx - 1) * len(categories) + cat_idx
                total_combos = len(countries) * len(categories)
                
                print(f"ğŸ” [{combo_num}/{total_combos}] {country['êµ­ê°€ëª…']} - {cat['ì¹´í…Œê³ ë¦¬ëª…']} ìˆ˜ì§‘ ì¤‘...")
                
                try:
                    # YouTube API í˜¸ì¶œ
                    v_url = (
                        f"https://www.googleapis.com/youtube/v3/videos"
                        f"?part=snippet,statistics,contentDetails"
                        f"&chart=mostPopular"
                        f"&regionCode={country['êµ­ê°€ì½”ë“œ']}"
                        f"&videoCategoryId={cat['ì¹´í…Œê³ ë¦¬ID']}"
                        f"&maxResults=50"
                        f"&key={current_key}"
                    )
                    v_res = requests.get(v_url, timeout=30).json()

                    if 'items' in v_res:
                        # ì±„ë„ ì •ë³´ ìˆ˜ì§‘
                        c_ids = [i['snippet']['channelId'] for i in v_res['items']]
                        c_url = (
                            f"https://www.googleapis.com/youtube/v3/channels"
                            f"?part=snippet,statistics"
                            f"&id={','.join(c_ids)}"
                            f"&key={current_key}"
                        )
                        c_res = requests.get(c_url, timeout=30).json()
                        
                        c_map = {
                            c['id']: {
                                'handle': c['snippet'].get('customUrl', 'N/A'),
                                'subs': c['statistics'].get('subscriberCount', 0)
                            }
                            for c in c_res.get('items', [])
                        }

                        # ì˜ìƒ ë°ì´í„° ì²˜ë¦¬
                        for idx, item in enumerate(v_res['items'], 1):
                            snip = item['snippet']
                            stat = item['statistics']
                            cdet = item['contentDetails']
                            c_id = snip['channelId']
                            c_info = c_map.get(c_id, {'handle': 'N/A', 'subs': 0})
                            
                            dur = parse_duration(cdet['duration'])
                            if dur <= 60:
                                d_type = "Shorts"
                            elif dur < 120:
                                d_type = "Mid-form"
                            else:
                                d_type = "Long-form"

                            tags = ", ".join(snip.get('tags', [])[:10]) if snip.get('tags') else ""

                            all_results.append([
                                datetime.now().strftime('%Y-%m-%d %H:%M'),
                                country['êµ­ê°€ëª…'],
                                cat['ì¹´í…Œê³ ë¦¬ëª…'],
                                d_type,
                                idx,
                                snip["thumbnails"]["medium"]["url"],
                                snip['title'],
                                int(stat.get('viewCount', 0)),
                                snip['channelTitle'],
                                c_info['handle'],
                                int(c_info['subs']),
                                tags,
                                f"https://www.youtube.com/watch?v={item['id']}",
                                c_id,
                                snip['thumbnails']['medium']['url']
                            ])
                        
                        success_count += 1
                        print(f"   âœ… {len(v_res['items'])}ê°œ ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ")
                    else:
                        fail_count += 1
                        print(f"   âš ï¸  ë°ì´í„° ì—†ìŒ")
                        key_idx += 1
                    
                    time.sleep(0.1)
                    
                except Exception as e:
                    fail_count += 1
                    print(f"   âŒ ì—ëŸ¬: {e}")
                    key_idx += 1

        # Turso DBì— ì €ì¥
        print("\n" + "=" * 60)
        print("ğŸ’¾ Turso DBì— ì €ì¥ ì¤‘...")
        print("=" * 60)
        
        if all_results:
            inserted = insert_to_turso(all_results)
            
            print("\n" + "=" * 60)
            print("âœ… DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_results)}ê°œ ì˜ìƒ")
            print(f"ğŸ’¾ DB ì €ì¥: {inserted}ê°œ")
            print(f"âœ… ì„±ê³µ: {success_count}ê°œ ì¡°í•©")
            print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ ì¡°í•©")
            print("=" * 60)
        else:
            print("âš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

# ========================================
# ì‹¤í–‰
# ========================================
if __name__ == '__main__':
    run_final_collector()
