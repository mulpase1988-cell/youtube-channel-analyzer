# scripts/step2_collect_youtube_data.py
"""
Step 2: YouTube ë°ì´í„° ìˆ˜ì§‘
- Step 1ì˜ channel_ids.json ì½ê¸°
- YouTube API + RSSë¡œ ì±„ë„ ì •ë³´ ìˆ˜ì§‘
- youtube_data.json ì €ì¥
"""

import os
import json
import logging
import time
import feedparser
from datetime import datetime, timezone
from dateutil import parser as dateutil_parser

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# ì„¤ì •
# ============================================================================

DATA_DIR = 'data'
CHANNEL_IDS_FILE = os.path.join(DATA_DIR, 'channel_ids.json')
YOUTUBE_DATA_FILE = os.path.join(DATA_DIR, 'youtube_data.json')

API_TAB_NAME = 'API_í‚¤_ê´€ë¦¬'
SHEET_NAME = 'ìœ íŠœë¸Œë³´ë¬¼ì°½ê³ _í…ŒìŠ¤íŠ¸'

# êµ­ê°€ ë§¤í•‘
COUNTRY_MAP = {
    'KR': 'í•œêµ­', 'JP': 'ì¼ë³¸', 'US': 'ë¯¸êµ­', 'GB': 'ì˜êµ­',
    'DE': 'ë…ì¼', 'FR': 'í”„ë‘ìŠ¤', 'VN': 'ë² íŠ¸ë‚¨', 'TH': 'íƒœêµ­',
    'ID': 'ì¸ë„ë„¤ì‹œì•„', 'IN': 'ì¸ë„', 'BR': 'ë¸Œë¼ì§ˆ', 'MX': 'ë©•ì‹œì½”',
    'CA': 'ìºë‚˜ë‹¤', 'AU': 'í˜¸ì£¼', 'RU': 'ëŸ¬ì‹œì•„', 'TR': 'í„°í‚¤',
    'ES': 'ìŠ¤í˜ì¸', 'IT': 'ì´íƒˆë¦¬ì•„', 'TW': 'ëŒ€ë§Œ', 'HK': 'í™ì½©', 'PH': 'í•„ë¦¬í•€'
}

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘
CATEGORY_MAP = {
    '1': 'ì˜í™”', '2': 'ìë™ì°¨', '10': 'ìŒì•…', '15': 'ë°˜ë ¤ë™ë¬¼',
    '17': 'ìŠ¤í¬ì¸ ', '18': 'ë‹¨í¸ì˜ìƒ', '19': 'ì—¬í–‰', '20': 'ê²Œì„',
    '21': 'ë¸”ë¡œê±°', '22': 'ì¸ë¬¼', '23': 'ì½”ë¯¸ë””', '24': 'ì—”í„°í…Œì¸ë¨¼íŠ¸',
    '25': 'ë‰´ìŠ¤', '26': 'êµìœ¡', '27': 'ê³¼í•™', '28': 'ê¸°ìˆ ', '29': 'ì‚¬íšŒ'
}

# ============================================================================
# API í‚¤ ë¡œë“œ
# ============================================================================

def load_api_keys_from_google_sheets():
    """Google Sheetsì—ì„œ API í‚¤ ë¡œë“œ"""
    import gspread
    from google.oauth2.service_account import Credentials
    
    service_account_json = os.getenv('GOOGLE_SERVICE_ACCOUNT')
    if not service_account_json:
        logger.error("âŒ GOOGLE_SERVICE_ACCOUNT í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        service_account_info = json.loads(service_account_json)
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []
    
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]
    
    credentials = Credentials.from_service_account_info(
        service_account_info,
        scopes=scopes
    )
    
    gc = gspread.authorize(credentials)
    
    try:
        spreadsheet = gc.open(SHEET_NAME)
        worksheet_api = spreadsheet.worksheet(API_TAB_NAME)
        logger.info(f"âœ… '{API_TAB_NAME}' ì›Œí¬ì‹œíŠ¸ ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        return []
    
    try:
        all_values = worksheet_api.get_all_values()
        
        if len(all_values) < 4:
            logger.warning("âš ï¸ API í‚¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        headers = all_values[2]
        
        try:
            idx_name = headers.index('í‚¤ ì´ë¦„')
            idx_key = headers.index('API í‚¤')
            idx_status = headers.index('í™œì„±í™”')
        except ValueError as e:
            logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return []
        
        api_keys = []
        
        for row_idx, row in enumerate(all_values[3:], start=4):
            if len(row) <= max(idx_name, idx_key, idx_status):
                continue
            
            key_name = row[idx_name].strip() if len(row) > idx_name else ""
            key_value = row[idx_key].strip() if len(row) > idx_key else ""
            status = row[idx_status].strip() if len(row) > idx_status else ""
            
            if key_value and status.upper() in ['TRUE', 'YES', 'O', 'í™œì„±í™”', 'ì‚¬ìš©']:
                api_keys.append({
                    'name': key_name,
                    'key': key_value
                })
                masked_key = key_value[:10] + '...' + key_value[-5:]
                logger.info(f"   âœ“ {key_name}: {masked_key}")
        
        logger.info(f"âœ… API í‚¤ {len(api_keys)}ê°œ ë¡œë“œ ì™„ë£Œ")
        return api_keys
    
    except Exception as e:
        logger.error(f"âŒ API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# ============================================================================
# RSS í”¼ë“œ íŒŒì‹±
# ============================================================================

def parse_rss_feed(channel_id, max_videos=15):
    """YouTube RSS í”¼ë“œì—ì„œ ìµœê·¼ ì˜ìƒ ì¶”ì¶œ"""
    
    rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    try:
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            return []
        
        videos = []
        for entry in feed.entries[:max_videos]:
            try:
                video_id = entry.yt_videoid if hasattr(entry, 'yt_videoid') else None
                if not video_id and 'id' in entry:
                    video_id = entry.id.split(':')[-1]
                
                published_str = entry.published if hasattr(entry, 'published') else None
                published_at = None
                
                if published_str:
                    try:
                        published_at = dateutil_parser.parse(published_str)
                        if published_at.tzinfo is None:
                            published_at = published_at.replace(tzinfo=timezone.utc)
                    except:
                        pass
                
                videos.append({
                    'video_id': video_id,
                    'title': entry.title if hasattr(entry, 'title') else '',
                    'published_at': published_at
                })
            except Exception as e:
                logger.warning(f"âš ï¸ RSS í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        return videos
    
    except Exception as e:
        logger.warning(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

# ============================================================================
# ì±„ë„ ë°ì´í„° ìˆ˜ì§‘
# ============================================================================

def get_channel_data(channel_id, api_key):
    """YouTube APIë¡œ ì±„ë„ ë°ì´í„° ìˆ˜ì§‘"""
    
    result = {
        'channel_id': channel_id,
        'channel_name': '',
        'handle': '',
        'country': 'í•œêµ­',
        'subscribers': 0,
        'video_count': 0,
        'total_views': 0,
        'first_upload': '',
        'latest_upload': '',
        'yt_category': 'ë¯¸ë¶„ë¥˜',
        'video_links': ['', '', '', '', ''],
        'views_5': 0,
        'views_10': 0,
        'views_20': 0,
        'views_30': 0,
        'views_5d': 0,
        'views_10d': 0,
        'views_15d': 0,
        'count_5d': 0,
        'count_10d': 0,
        'operation_days': 0,
        'collect_date': datetime.now(timezone.utc).strftime('%Y-%m-%d')
    }
    
    try:
        youtube = build('youtube', 'v3', developerKey=api_key)
        
        # ì±„ë„ ì •ë³´ ì¡°íšŒ
        logger.info(f"   ğŸ“¡ ì±„ë„ ì •ë³´ ì¡°íšŒ ì¤‘...")
        channel_response = youtube.channels().list(
            part='snippet,statistics,contentDetails',
            id=channel_id
        ).execute()
        
        if not channel_response.get('items'):
            logger.error(f"   âŒ ì±„ë„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        channel_info = channel_response['items'][0]
        snippet = channel_info['snippet']
        statistics = channel_info['statistics']
        
        result['channel_name'] = snippet.get('title', '')
        result['handle'] = snippet.get('customUrl', '')
        
        country_code = snippet.get('country', '').strip()
        if country_code:
            result['country'] = COUNTRY_MAP.get(country_code, country_code)
        
        result['subscribers'] = int(statistics.get('subscriberCount', 0))
        result['video_count'] = int(statistics.get('videoCount', 0))
        result['total_views'] = int(statistics.get('viewCount', 0))
        
        logger.info(f"   âœ“ ì±„ë„: {result['channel_name']}")
        logger.info(f"   âœ“ êµ¬ë…ì: {result['subscribers']:,} | ì˜ìƒ: {result['video_count']:,} | ì¡°íšŒìˆ˜: {result['total_views']:,}")
        
        # ì±„ë„ ê°œì„¤ì¼
        channel_created = snippet.get('publishedAt', '')
        
        # RSS í”¼ë“œì—ì„œ ì˜ìƒ ì •ë³´ ìˆ˜ì§‘
        logger.info(f"   ğŸ“¥ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
        rss_videos = parse_rss_feed(channel_id, max_videos=15)
        logger.info(f"   âœ“ RSSì—ì„œ {len(rss_videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘")
        
        # APIì—ì„œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì˜ìƒ ìˆ˜ì§‘
        uploads_playlist_id = channel_info['contentDetails']['relatedPlaylists']['uploads']
        
        try:
            logger.info(f"   ğŸ“¥ ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
            playlist_response = youtube.playlistItems().list(
                part='contentDetails',
                playlistId=uploads_playlist_id,
                maxResults=30
            ).execute()
            
            api_videos = []
            for item in playlist_response.get('items', []):
                try:
                    video_id = item['contentDetails']['videoId']
                    api_videos.append(video_id)
                except:
                    continue
            
            logger.info(f"   âœ“ APIì—ì„œ {len(api_videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘")
        
        except HttpError as e:
            if e.resp.status == 404:
                logger.warning(f"   âš ï¸ ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì—†ìŒ (Shorts ì±„ë„?)")
                api_videos = []
            else:
                raise
        
        # ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ (RSS + API)
        all_video_ids = [v['video_id'] for v in rss_videos if v['video_id']] + api_videos
        all_video_ids = list(dict.fromkeys(all_video_ids))[:30]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 30ê°œ
        
        if all_video_ids:
            logger.info(f"   ğŸ“º ì˜ìƒ ì •ë³´ ì¡°íšŒ ì¤‘ ({len(all_video_ids)}ê°œ)...")
            videos_response = youtube.videos().list(
                part='statistics,snippet',
                id=','.join(all_video_ids)
            ).execute()
            
            # ì¹´í…Œê³ ë¦¬ ì„¤ì •
            if videos_response.get('items'):
                try:
                    first_category_id = videos_response['items'][0]['snippet'].get('categoryId', '')
                    result['yt_category'] = CATEGORY_MAP.get(first_category_id, 'ë¯¸ë¶„ë¥˜')
                except:
                    pass
            
            # ì˜ìƒ ë§í¬ ì €ì¥
            for i, item in enumerate(videos_response.get('items', [])[:5]):
                try:
                    result['video_links'][i] = f"https://www.youtube.com/watch?v={item['id']}"
                except:
                    pass
            
            # ì¡°íšŒìˆ˜ ê³„ì‚°
            view_data = {}
            now = datetime.now(timezone.utc)
            
            for video in videos_response.get('items', []):
                try:
                    video_id = video['id']
                    view_count = int(video['statistics'].get('viewCount', 0))
                    published_str = video['snippet'].get('publishedAt', '')
                    
                    published_at = None
                    if published_str:
                        try:
                            published_at = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                        except:
                            pass
                    
                    view_data[video_id] = {
                        'views': view_count,
                        'published_at': published_at
                    }
                except:
                    continue
            
            # ì¼ìë³„ ì¡°íšŒìˆ˜ í•©ê³„
            views_list = [v['views'] for v in view_data.values()]
            result['views_5'] = sum(views_list[:5])
            result['views_10'] = sum(views_list[:10])
            result['views_20'] = sum(views_list[:20])
            result['views_30'] = sum(views_list[:30])
            
            views_5d = []
            views_10d = []
            views_15d = []
            dates = []
            
            for video_id, data in view_data.items():
                pub_at = data['published_at']
                if not pub_at:
                    continue
                
                dates.append(pub_at)
                days_ago = (now - pub_at).days
                
                if days_ago <= 5:
                    views_5d.append(data['views'])
                if days_ago <= 10:
                    views_10d.append(data['views'])
                if days_ago <= 15:
                    views_15d.append(data['views'])
            
            result['views_5d'] = sum(views_5d)
            result['views_10d'] = sum(views_10d)
            result['views_15d'] = sum(views_15d)
            result['count_5d'] = len(views_5d)
            result['count_10d'] = len(views_10d)
            
            # ìµœì´ˆ/ìµœê·¼ ì—…ë¡œë“œ
            if dates:
                result['latest_upload'] = max(dates).strftime('%Y-%m-%d')
                result['first_upload'] = min(dates).strftime('%Y-%m-%d')
                result['operation_days'] = (now - min(dates)).days
            elif channel_created:
                result['first_upload'] = channel_created[:10]
                try:
                    created_date = datetime.fromisoformat(channel_created.replace('Z', '+00:00'))
                    result['operation_days'] = (now - created_date).days
                except:
                    pass
            
            logger.info(f"   âœ… 5ì¼: {result['views_5d']:,}íšŒ ({result['count_5d']}ê°œ)")
            logger.info(f"   âœ… 10ì¼: {result['views_10d']:,}íšŒ ({result['count_10d']}ê°œ)")
            logger.info(f"   âœ… 15ì¼: {result['views_15d']:,}íšŒ")
        
        return result
    
    except Exception as e:
        logger.error(f"   âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return None

# ============================================================================
# Step 2 ë©”ì¸ í•¨ìˆ˜
# ============================================================================

def process_step2():
    """Step 2: YouTube ë°ì´í„° ìˆ˜ì§‘"""
    
    logger.info("=" * 80)
    logger.info("ğŸš€ Step 2: YouTube ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 80)
    
    try:
        # [1/4] channel_ids.json ë¡œë“œ
        logger.info("\n[1/4] channel_ids.json ë¡œë“œ ì¤‘...")
        
        if not os.path.exists(CHANNEL_IDS_FILE):
            logger.error(f"âŒ {CHANNEL_IDS_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            logger.error("   Step 1ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”")
            return
        
        with open(CHANNEL_IDS_FILE, 'r', encoding='utf-8') as f:
            channel_ids_data = json.load(f)
        
        logger.info(f"âœ… {len(channel_ids_data)}ê°œ ì±„ë„ ë¡œë“œ ì™„ë£Œ")
        
        # [2/4] API í‚¤ ë¡œë“œ
        logger.info("\n[2/4] API í‚¤ ë¡œë“œ ì¤‘...")
        api_keys = load_api_keys_from_google_sheets()
        
        if not api_keys:
            logger.error("âŒ API í‚¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        api_key = api_keys[0]['key']
        
        # [3/4] ë°ì´í„° ìˆ˜ì§‘
        logger.info("\n[3/4] ì±„ë„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        youtube_data = []
        success_count = 0
        fail_count = 0
        
        for idx, channel_info in enumerate(channel_ids_data, 1):
            channel_id = channel_info['channel_id']
            channel_name = channel_info['channel_name']
            
            logger.info(f"\nâ–¶ [{idx}/{len(channel_ids_data)}] {channel_name}")
            
            data = get_channel_data(channel_id, api_key)
            
            if data:
                youtube_data.append(data)
                success_count += 1
            else:
                fail_count += 1
            
            # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ëŒ€ë¹„
            time.sleep(1)
        
        # [4/4] ê²°ê³¼ ì €ì¥
        logger.info("\n[4/4] ê²°ê³¼ ì €ì¥ ì¤‘...")
        os.makedirs(DATA_DIR, exist_ok=True)
        
        with open(YOUTUBE_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(youtube_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥: {YOUTUBE_DATA_FILE}")
        
        # ìš”ì•½
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š Step 2 ì™„ë£Œ ìš”ì•½")
        logger.info("=" * 80)
        logger.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {fail_count}ê°œ")
        logger.info(f"ğŸ“ ì €ì¥ íŒŒì¼: {YOUTUBE_DATA_FILE}")
        logger.info("=" * 80)
    
    except Exception as e:
        logger.error(f"\nâŒ Step 2 ì‹¤íŒ¨: {e}", exc_info=True)

# ============================================================================
# ì‹¤í–‰
# ============================================================================

if __name__ == '__main__':
    process_step2()
